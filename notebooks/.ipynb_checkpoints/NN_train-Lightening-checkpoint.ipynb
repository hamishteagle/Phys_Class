{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "sys.argv = [\"foo\",\"-r\",\"mini\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Driver       :: INFO     ::       Setting up Driver from /nfs/dust/atlas/user/hteagle/PyAnalysisUtils/Driver.py\n",
      "Driver       :: INFO     ::       NO VARIABLES SUPPLIED ON THE COMMAND LINE, JUST DRAWING ONE BIN\n",
      "Driver       :: INFO     ::       Variables from the command line...\n",
      "Driver       :: INFO     ::       \t {'var': '1', 'nbins': 1, 'xmin': 0.0, 'xmax': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import Driver\n",
    "from dataset import bbMeT_NN\n",
    "from Driver import logging\n",
    "msg=logging.getLogger(\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_driver\n",
    "NN_driver.load_NN_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DatasetLoader :: INFO     ::       Attempt to load:/nfs/dust/atlas/user/hteagle/PyAnalysisUtils/ML/NN/data/processed/data_Wh_21_2_112_mini_reco_sig.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n",
      "desy path: /nfs/dust/atlas/user/hteagle/Wh_samples/Wh_21.2.112/MC/\n",
      "['mini']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = bbMeT_NN(root='data', device=device, transform=None)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = NN_driver.num_workers\n",
    "batch_size = NN_driver.batch_size\n",
    "split_fraction = NN_driver.split_fraction\n",
    "epochs = NN_driver.epochs\n",
    "lr = NN_driver.lr\n",
    "weight_decay=NN_driver.weight_decay\n",
    "class_names=NN_driver.class_names\n",
    "num_classes = len(NN_driver.class_names)\n",
    "doReWeight=NN_driver.doReWeight\n",
    "doWeighted=NN_driver.doWeighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "# indices = list(range(len(dataset)))\n",
    "# split = int(np.floor(split_fraction * len(dataset)))\n",
    "# train_indices,test_indices = indices[:split], indices[split:]\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modules.CatL(X_size=len(dataset.X_data[0]),num_classes=num_classes).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss( weight = class_weights)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr , weight_decay=weight_decay)\n",
    "model=model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New output:current_log/2020-08-18/16:51:35.554416\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size,sampler=train_sampler, drop_last=True)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size,sampler=test_sampler, drop_last=False)\n",
    "#%load_ext tensorboard\n",
    "import XGB_board\n",
    "xmini,ymini,wmini = next(iter(train_loader))\n",
    "board = XGB_board.board_object(model,xmini=xmini, device=device, output_dir=\"current_log/\")\n",
    "#%tensorboard --logdir board.save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skorch's way of running things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skorch import NeuralNetClassifier\n",
    "#net = NeuralNetClassifier(model, max_epochs=epochs, criterion = torch.nn.CrossEntropyLoss( weight = class_weights), optimizer__lr = lr, device=device, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 60287), started 20:38:35 ago. (Use '!kill 60287' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ad059d9e10dcdd47\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ad059d9e10dcdd47\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#net.fit(X_train, Y_train)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir  current_log/2020-08-17/ --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]../modules.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = torch.nn.functional.softmax(x)## When training/calculating losses using CrossEntropyLoss we should feed it raw logits, it applies softmax itself. When applying/getting probabilites, we should switch this on..annoying\n",
      "100%|██████████| 5/5 [01:19<00:00, 15.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CatL(\n",
       "  (dropout10): AlphaDropout(p=0.1, inplace=False)\n",
       "  (dropout20): AlphaDropout(p=0.2, inplace=False)\n",
       "  (dropout30): AlphaDropout(p=0.3, inplace=False)\n",
       "  (linear_input): Linear(in_features=35, out_features=140, bias=True)\n",
       "  (linear1): Linear(in_features=140, out_features=140, bias=True)\n",
       "  (linear2): Linear(in_features=140, out_features=140, bias=True)\n",
       "  (linear3): Linear(in_features=140, out_features=140, bias=True)\n",
       "  (linear4): Linear(in_features=140, out_features=140, bias=True)\n",
       "  (linear5): Linear(in_features=140, out_features=140, bias=True)\n",
       "  (linear_output): Linear(in_features=140, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    start = datetime.now()\n",
    "    model.train()\n",
    "    for local_batch, local_labels, local_weights in train_loader:\n",
    "        local_batch, local_labels, local_weights = local_batch.to(device), local_labels.to(device), local_weights.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        if doWeighted:\n",
    "            criterion = torch.nn.CrossEntropyLoss( weight = ut.get_batch_weights(local_labels,local_weights,(len(class_names)),doMonoSigWgt))\n",
    "        elif doReWeight:\n",
    "            criterion = torch.nn.CrossEntropyLoss( weight = class_weights)\n",
    "        out = model(local_batch.float())\n",
    "        loss = criterion(out, local_labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    epoch_time = datetime.now()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #msg.info(\"Calculating Epoch losses:\")\n",
    "        if doWeighted:\n",
    "            valid_loss = 0\n",
    "            train_loss = 0\n",
    "            for xb, yb, wb in test_loader:\n",
    "                criterion = torch.nn.CrossEntropyLoss( weight = ut.get_batch_weights(yb,wb,(len(class_names)),doMonoSigWgt))\n",
    "                valid_loss = valid_loss + criterion(model(xb.float()), yb)\n",
    "            for xb, yb, wb in train_loader:\n",
    "                criterion = torch.nn.CrossEntropyLoss( weight = ut.get_batch_weights(yb,wb,(len(class_names)),doMonoSigWgt))\n",
    "                train_loss = train_loss + criterion(model(xb.float()), yb)\n",
    "        else:\n",
    "            valid_loss = sum(criterion(model(xb.float()), yb) for xb, yb, wb in test_loader)\n",
    "            train_loss = sum(criterion(model(xb.float()), yb) for xb, yb, wb in train_loader)\n",
    "    test_loss = valid_loss/len(test_loader)\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    loss_time = datetime.now()\n",
    "    board.writer.add_scalars(\"Losses\",{\"validation_loss/epoch\":test_loss,\n",
    "                                        \"train_loss/epoch\":train_loss,},epoch)\n",
    "    #board.writer.add_scalar(\"learning_rate\",lr, epoch)\n",
    "    #msg.info(\"Train loss: %f, test loss %f\" %(train_loss, test_loss))\n",
    "    #msg.info(\"Time: epoch: %f, loss calculation: %f\" %((loss_time-epoch_time).total_seconds(), (epoch_time-start).total_seconds()))\n",
    "model.eval() #Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils as ut\n",
    "ut.save_model(model, board.save_dir,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.get_inputs(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = board.draw_train_test_root()\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curves per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "importlib.reload(XGB_board)\n",
    "#new_board = XGB_board.board_object(model,xmini=xmini, device=device, output_dir=\"current_log/\")\n",
    "#new_board.get_inputs(train_loader, test_loader)\n",
    "auc, curve = board.get_roc(class_ = \"Signal\")\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve.ax_.legend(\"Signal ROC, ACU: %f\"%(auc))\n",
    "curve.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_params={}\n",
    "h_params[\"learning_rate\"] = NN_driver.lr\n",
    "h_params[\"batch_size\"] = NN_driver.batch_size\n",
    "h_params[\"doWeighted\"] = NN_driver.doWeighted\n",
    "h_params[\"doReWeight\"] = NN_driver.doReWeight\n",
    "h_params[\"split_fraction\"] = NN_driver.split_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_params = {}\n",
    "metric_params[\"Signal_AUC\"]=auc\n",
    "metric_params[\"PValue_bkg\"]=board.pval_bkg_KS\n",
    "#metric_params[\"PValue_bkg\"]=0.5688432194575677\n",
    "metric_params[\"PValue_sig\"]=board.pval_sig_KS\n",
    "#metric_params[\"PValue_sig\"]=0.3728415656927149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.writer.add_hparams(h_params, metric_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_load = [\"C1N2_Wh_300.0_150.0\", \"C1N2_Wh_350.0_200.0\", \"C1N2_Wh_400.0_250.0\", \"C1N2_Wh_450.0_300.0\"]\n",
    "signal_samples = [ut.load_numpy(dataset, NN_driver, single_sample = sam) for sam in to_load]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
